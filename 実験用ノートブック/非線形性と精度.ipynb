{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5add2237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] name\n",
      "ipykernel_launcher.py: error: the following arguments are required: name\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from logging import INFO, FileHandler, getLogger\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np  # 追加\n",
    "import yaml\n",
    "from tqdm import tqdm \n",
    "from config.config import Config\n",
    "from config.config_logger import record_config_to_cfg, record_value_to_cfg\n",
    "from src.data_collaboration import DataCollaborationAnalysis\n",
    "from src.institutional_analysis import (\n",
    "    centralize_analysis,\n",
    "    centralize_analysis_with_dimension_reduction,\n",
    "    dca_analysis,\n",
    "    individual_analysis,\n",
    "    fl_analysis,\n",
    "    individual_analysis_with_dimension_reduction,\n",
    ")\n",
    "from src.load_data import load_data\n",
    "from src.paths import CONFIG_DIR, INPUT_DIR, OUTPUT_DIR\n",
    "\n",
    "# 引数の設定\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"name\", type=str, default=\"exp001\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# yaml のパスと出力先パス\n",
    "cfg_path    = CONFIG_DIR / f\"{args.name}.yaml\"\n",
    "output_path = OUTPUT_DIR / args.name\n",
    "\n",
    "# UTF-8 で読み込んで Config を生成\n",
    "with cfg_path.open(encoding=\"utf-8\") as f:\n",
    "    cfg_dict = yaml.safe_load(f)\n",
    "\n",
    "config = Config(**cfg_dict,\n",
    "                output_path=output_path,\n",
    "                input_path=INPUT_DIR)\n",
    "\n",
    "# 出力ディレクトリ作成\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ログの設定\n",
    "logger = getLogger(__name__)\n",
    "logger.setLevel(INFO)\n",
    "handler = FileHandler(filename=config.output_path / \"result.log\", encoding=\"utf-8\")\n",
    "logger.addHandler(handler)\n",
    "\n",
    "def main(visualize):\n",
    "    logger.info(f\"データセット: {config.dataset}\")\n",
    "    print(f\"データセット:{config.dataset}\")\n",
    "    config.f_seed = 0\n",
    "    \n",
    "    # datasetの読み込み\n",
    "    train_df, test_df = load_data(config=config)\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    \n",
    "    if config.F_type == \"kernel_pca\" and config.G_type == \"GEP_weighted\":\n",
    "        # GEP_weightedはUSE_KERNELがTrueのときのみ実行\n",
    "        return\n",
    "    #if config.F_type == \"kernel_pca\" and config.G_type == \"GEP\":\n",
    "        # GEP_weightedはUSE_KERNELがTrueのときのみ実行\n",
    "    #    return\n",
    "    config.log(logger, exclude_keys=[\"output_path\", \"input_path\", \"name\", \"seed\", \"y_name\"])\n",
    "    # インスタンスの生成\n",
    "    data_collaboration = DataCollaborationAnalysis(config=config, logger=logger, train_df=train_df, test_df=test_df)\n",
    "    # データ分割 -> 統合表現の獲得まで一気に実行\n",
    "    #data_collaboration.save_optimal_params()\n",
    "    data_collaboration.run()\n",
    "    if visualize:\n",
    "        data_collaboration.visualize_representations()\n",
    "        print(1111)\n",
    "    #data_collaboration.save_representations_to_csv()\n",
    "        # 提案手法\n",
    "    #record_config_to_cfg(config)\n",
    "    if config.G_type == 'centralize':\n",
    "                # 集中解析\n",
    "        metrics_cen = centralize_analysis(config, logger, y_name=config.y_name)\n",
    "        metrics_dict['centralize'] = metrics_cen\n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", metrics_cen)\n",
    "        return metrics_cen\n",
    "    \n",
    "    elif config.G_type == 'centralize_dim':\n",
    "        # 集中解析 with 次元削減\n",
    "        metrics_cen_dim = centralize_analysis_with_dimension_reduction(config, logger, y_name=config.y_name)\n",
    "        metrics_dict['centralize_dim'] = metrics_cen_dim\n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", metrics_cen_dim)\n",
    "        return metrics_cen_dim\n",
    "    \n",
    "    elif config.G_type == 'individual':\n",
    "        # 個別解析\n",
    "        metrics_ind = individual_analysis(\n",
    "            config=config,\n",
    "            logger=logger,\n",
    "            Xs_train=data_collaboration.Xs_train,\n",
    "            ys_train=data_collaboration.ys_train,\n",
    "            Xs_test=data_collaboration.Xs_test,\n",
    "            ys_test=data_collaboration.ys_test,\n",
    "        )\n",
    "        #metrics_dict['individual'] = metrics_ind\n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", metrics_ind)\n",
    "        return metrics_ind\n",
    "    \n",
    "    elif config.G_type == 'individual_dim':\n",
    "        # 個別解析 with 次元削減\n",
    "        metrics_ind_dim = individual_analysis_with_dimension_reduction(\n",
    "            config=config,\n",
    "            logger=logger,\n",
    "            Xs_train=data_collaboration.Xs_train,\n",
    "            ys_train=data_collaboration.ys_train,\n",
    "            Xs_test=data_collaboration.Xs_test,\n",
    "            ys_test=data_collaboration.ys_test,\n",
    "        )\n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", metrics_ind_dim)\n",
    "        return metrics_ind_dim\n",
    "    \n",
    "    elif config.G_type == 'fl':\n",
    "        metrics_fl = fl_analysis(\n",
    "            config=config,\n",
    "            logger=logger,\n",
    "            Xs_train=data_collaboration.Xs_train,\n",
    "            ys_train=data_collaboration.ys_train,\n",
    "            Xs_test=data_collaboration.Xs_test,\n",
    "            ys_test=data_collaboration.ys_test,\n",
    "        )\n",
    "        metrics_dict['fl'] = metrics_fl\n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", metrics_fl)\n",
    "        return metrics_fl\n",
    "    else:\n",
    "        # metrics_ind_dim = individual_analysis_with_dimension_reduction(\n",
    "        #     config=config,\n",
    "        #     logger=logger,\n",
    "        #     Xs_train=data_collaboration.Xs_train,\n",
    "        #     ys_train=data_collaboration.ys_train,\n",
    "        #     Xs_test=data_collaboration.Xs_test,\n",
    "        #     ys_test=data_collaboration.ys_test,\n",
    "        # )\n",
    "        # config.num_institution\n",
    "        # config.num_institution_user\n",
    "        metrics = dca_analysis(\n",
    "                        X_train_integ=data_collaboration.X_train_integ,\n",
    "                        X_test_integ=data_collaboration.X_test_integ,\n",
    "                        y_train_integ=data_collaboration.y_train_integ,\n",
    "                        y_test_integ=data_collaboration.y_test_integ,\n",
    "                        config=config,\n",
    "                        logger=logger,\n",
    "                    )\n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", metrics)\n",
    "        return metrics\n",
    "        \n",
    "        # --- ここから機関ごとの metrics を算出 ---\n",
    "        # 各機関のサンプル数（元リスト）から、統合後配列のスライス境界を作る\n",
    "        train_counts = [len(y) for y in data_collaboration.ys_train]\n",
    "        test_counts  = [len(y) for y in data_collaboration.ys_test]\n",
    "        n_inst = min(config.num_institution, len(train_counts))\n",
    "\n",
    "        train_cum = np.concatenate(([0], np.cumsum(train_counts)))\n",
    "        test_cum  = np.concatenate(([0], np.cumsum(test_counts)))\n",
    "\n",
    "        inst_losses = []\n",
    "        even_losses = []\n",
    "        odd_losses = []\n",
    "        \n",
    "        for i in range(n_inst):\n",
    "            # 各機関の訓練・テストから num_institution_user 件だけ使用\n",
    "            tr_start, tr_end = int(train_cum[i]), int(train_cum[i+1])\n",
    "            te_start, te_end = int(test_cum[i]),  int(test_cum[i+1])\n",
    "\n",
    "            tr_take = min(config.num_institution_user, tr_end - tr_start)\n",
    "            te_take = min(config.num_institution_user, te_end - te_start)\n",
    "\n",
    "            X_tr_i = data_collaboration.X_train_integ[tr_start: tr_start + tr_take, :]\n",
    "            y_tr_i = data_collaboration.y_train_integ[tr_start: tr_start + tr_take]\n",
    "            X_te_i = data_collaboration.X_test_integ[te_start:  te_start  + te_take,  :]\n",
    "            y_te_i = data_collaboration.y_test_integ[te_start:  te_start  + te_take]\n",
    "\n",
    "\n",
    "            metric_i = dca_analysis(\n",
    "                X_train_integ=data_collaboration.X_train_integ,\n",
    "                X_test_integ=X_te_i,\n",
    "                y_train_integ=data_collaboration.y_train_integ,\n",
    "                y_test_integ=y_te_i,\n",
    "                config=config,\n",
    "                logger=logger,\n",
    "            )\n",
    "            inst_losses.append(metric_i)\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                even_losses.append(metric_i)\n",
    "            else:\n",
    "                odd_losses.append(metric_i)\n",
    "\n",
    "        # 平均・最小・最大を算出して出力\n",
    "        inst_losses = np.array(inst_losses, dtype=float)\n",
    "        mean_val = float(inst_losses.mean())\n",
    "        min_val  = float(inst_losses.min())\n",
    "        max_val  = float(inst_losses.max())\n",
    "        \n",
    "        config.losses_mean = round(mean_val, 4)\n",
    "        config.losses_even =  round(sum(even_losses)/len(even_losses), 4)\n",
    "        config.losses_odd = round(sum(odd_losses)/len(odd_losses), 4)\n",
    "        #record_config_to_cfg(config)\n",
    "        print(\"11111111111111111111111111111111111111111111111111111111\")\n",
    "        print(\"len(even_losses)\",  len(even_losses))\n",
    "        \n",
    "        \n",
    "        record_config_to_cfg(config)\n",
    "        record_value_to_cfg(config, \"評価値\", mean_val)\n",
    "        print(\"評価値\", mean_val)\n",
    "        print(\"config.losses_mean\", config.losses_mean)\n",
    "        print(f\"機関ごとの {config.metrics}: {np.round(inst_losses, 4).tolist()}\")\n",
    "        print(f\"平均: {mean_val:.4f}, 最小: {min_val:.4f}, 最大: {max_val:.4f}\")\n",
    "        logger.info(f\"機関ごとの {config.metrics}: {inst_losses.tolist()}\")\n",
    "        logger.info(f\"平均: {mean_val:.6f}, 最小: {min_val:.6f}, 最大: {max_val:.6f}\")\n",
    "\n",
    "        # main_loop の集計用に平均値を返す\n",
    "        return mean_val    \n",
    "    \n",
    "    \n",
    "    # 個別解析\n",
    "    # metrics_ind = individual_analysis_with_dimension_reduction(\n",
    "    #     config=config,\n",
    "    #     logger=logger,\n",
    "    #     Xs_train=data_collaboration.Xs_train,\n",
    "    #     ys_train=data_collaboration.ys_train,\n",
    "    #     Xs_test=data_collaboration.Xs_test,\n",
    "    #     ys_test=data_collaboration.ys_test,\n",
    "    # )\n",
    "    #metrics_dict['individual_dim'] = metrics_ind\n",
    "    \n",
    "        # 個別解析 2 \n",
    "    # individual_analysis(\n",
    "    #     config=config,\n",
    "    #     logger=logger,\n",
    "    #     Xs_train=data_collaboration.Xs_train_inter,\n",
    "    #     ys_train=data_collaboration.ys_train,\n",
    "    #     Xs_test=data_collaboration.Xs_test_inter,\n",
    "    #     ys_test=data_collaboration.ys_test,\n",
    "    # )\n",
    "    #return metrics_dict \n",
    "\n",
    "def main_loop():\n",
    "    LOADERS = [\n",
    "    #    \"concentric_three_circles\",\n",
    "    #    \"mice\",\n",
    "    #  \"statlog\",\n",
    "        'qsar',\n",
    "    #   \"breast_cancer\",\n",
    "    #    \"adult\",\n",
    "    #    \"digits\",\n",
    "    #    \"concentric_circles\",\n",
    "    #    \"har\",\n",
    "    #    \"diabetes130\",\n",
    "    #    \"bank_marketing\", # 性能に変化でない\n",
    "    #    \"two_gaussian_distributions\",\n",
    "    #    '3D_gaussian_clusters',\n",
    "    #    \"3D_8_gaussian_clusters\",\n",
    "    #\"digits_v2\",\n",
    "    #\"housing\",\n",
    "    #\"ames\",\n",
    "    #\"tox21_sr_are\",\n",
    "    #\"hiv\",\n",
    "    #\"cyp3a4\",\n",
    "    #\"cyp2d6\",\n",
    "    #\"cyp1a2\",\n",
    "    #\"mnist\",\n",
    "    #\"fashion_mnist\",\n",
    "    ]\n",
    "    MODELS = [\"mlp\"]#, \"mlp\"]#\"random_forest\"]#, \"svm_linear_classifier\", \"mlp\"]#\"mlp\"]#, \"svm_linear_classifier\"] #\"svm_classifier\"]#\"random_forest\"]#, _linear_\n",
    "    gamma_types = [\"X_tuning\"] \n",
    "    F_types = [\"kernel_pca_svd_mixed\"]#\"kernel_pca_svd_mixed\", \"svd\", \"kernel_pca_self_tuning\"]#, \"svd\", \"kernel_pca_self_tuning\"]#\"svd\", \"kernel_pca\", \"kernel_pca_self_tuning\", ] # , \"kernel_pca\", \"lpp\" # \"kernel_pca_self_tuning\" \"kernel_pca_svd_mixed\",\n",
    "   #G_types = ['centralize', \"individual\", \"Imakura\", \"GEP\",  \"ODC\", \"nonlinear\"]#, 'centralize', \"individual\", \"Imakura\", \"GEP\",  \"ODC\", \"nonlinear\"]# \"nonlinear_tuning\"#'centralize_dim', \"nonlinear\", \"Imakura\"]#\"nonlinear_tuning\"]#, \"nonlinear\", \"nonlinear_tuning\", \"nonlinear_linear\"]#[\"fl\", 'centralize', 'individual', \"Imakura\", \"ODC\", \"GEP\", \"nonlinear\", \"nonlinear_tuning\", \"nonlinear_linear\"]#'centralize_dim', \"nonlinear\", \"Imakura\"]#\n",
    "    G_types = [\"nonlinear\"] # \"nonlinear\", \"Imakura\", \"GEP\",  \n",
    "    config.F_type = F_types[0]\n",
    "    F_type = F_types[0]\n",
    "    config.True_F_type = F_types[0]\n",
    "    config.G_type = G_types[0]\n",
    "    config.objective_direction_ratio = 0\n",
    "    config.gamma_type = \"X_tuning\"\n",
    "    config.lambda_pred = 0#10\n",
    "    config.lambda_offdiag = 0#100000\n",
    "    config.h_model = MODELS[0]\n",
    "    config.nl_lambda = 0.1\n",
    "    visualize = False\n",
    "    data = {}\n",
    "    model = MODELS[0]\n",
    "    config.losses_even_ind = 0\n",
    "    config.losses_odd_ind = 0\n",
    "    config.losses_ind = 0\n",
    "    config.losses_mean = 0\n",
    "    config.losses_even = 0\n",
    "    config.losses_odd = 0\n",
    "    config.integ_metrics = 0\n",
    "    for dataset in tqdm(LOADERS):\n",
    "        config.now = \"f\"\n",
    "        for met in [\"auc\"]:#, \"accuracy\"\n",
    "            config.metrics = met\n",
    "            config.h_model = model\n",
    "            print(dataset)\n",
    "            config.dataset = dataset\n",
    "            for gamma_ratio in [1]:#0.1, 1, 5]: # [0.01, 0.1, 1, 10, 100]\n",
    "                F_type = F_types[0]\n",
    "                config.gamma_ratio = gamma_ratio\n",
    "                config.F_type = F_type\n",
    "                config.True_F_type = F_type\n",
    "                for G_type in G_types:\n",
    "                    config.G_type = G_type\n",
    "                    for lw_alpha in [0]:\n",
    "                        config.lw_alpha = lw_alpha\n",
    "                        config.lb_beta = lw_alpha\n",
    "                        semi_integ = False\n",
    "                        orth = False\n",
    "                        config.semi_integ = semi_integ\n",
    "                        config.orth_ver = orth\n",
    "                        metrics = []\n",
    "                        losses_even_ind_list = []\n",
    "                        losses_odd_ind_list = []\n",
    "                        losses_ind_list = []\n",
    "                        losses_mean_list = []\n",
    "                        losses_even_list = []\n",
    "                        losses_odd_list = []\n",
    "                        integ_metrics_list = []\n",
    "                        \n",
    "                        if G_type == \"centralize\" or  \"individual\":\n",
    "                            losses_even_ind_list = [0]\n",
    "                            losses_odd_ind_list = [0]\n",
    "                            losses_ind_list = [0]\n",
    "                            losses_mean_list = [0]\n",
    "                            losses_even_list = [0]\n",
    "                            losses_odd_list = [0]\n",
    "                            integ_metrics_list = [0]\n",
    "\n",
    "                        for i in range(3, 15):\n",
    "                            config.seed = i\n",
    "                            #config.f_seed = i\n",
    "                            config.plot_name = f\"_0912_{dataset}_{G_type}.png\" # {self.config.lambda_pred}_{self.config.dataset}\n",
    "                            print(\"i\", i, \"G_type:\", G_type)\n",
    "                            metrics.append(main(visualize))\n",
    "                            losses_even_ind_list.append(config.losses_even_ind)\n",
    "                            losses_odd_ind_list.append(config.losses_odd_ind)\n",
    "                            losses_ind_list.append(config.losses_ind)\n",
    "                            losses_mean_list.append(config.losses_mean)\n",
    "                            losses_even_list.append(config.losses_even)\n",
    "                            losses_odd_list.append(config.losses_odd)\n",
    "                            integ_metrics_list.append(config.integ_metrics)\n",
    "                            config.F_type = config.True_F_type\n",
    "                        # 平均値を計算\n",
    "                        metrics_mean = sum(metrics) / len(metrics)\n",
    "                        metrics_stdev = statistics.stdev(metrics) if len(metrics) > 1 else 0.0\n",
    "                        losses_even_ind_mean = sum(losses_even_ind_list) / len(losses_even_ind_list)\n",
    "                        losses_odd_ind_mean = sum(losses_odd_ind_list) / len(losses_odd_ind_list)\n",
    "                        losses_ind_mean = sum(losses_ind_list) / len(losses_ind_list)\n",
    "                        losses_mean_mean = sum(losses_mean_list) / len(losses_mean_list)\n",
    "                        losses_even_mean = sum(losses_even_list) / len(losses_even_list)\n",
    "                        losses_odd_mean = sum(losses_odd_list) / len(losses_odd_list)\n",
    "                        integ_metrics_mean = sum(integ_metrics_list) / len(integ_metrics_list)\n",
    "                        losses_even_ind_stdev = statistics.stdev(losses_even_ind_list) if len(losses_even_ind_list) > 1 else 0.0\n",
    "                        losses_odd_ind_stdev = statistics.stdev(losses_odd_ind_list) if len(losses_odd_ind_list) > 1 else 0.0\n",
    "                        losses_ind_stdev = statistics.stdev(losses_ind_list) if len(losses_ind_list) > 1 else 0.0\n",
    "                        losses_mean_stdev = statistics.stdev(losses_mean_list) if len(losses_mean_list) > 1 else 0.0\n",
    "                        losses_even_stdev = statistics.stdev(losses_even_list) if len(losses_even_list) > 1 else 0.0\n",
    "                        losses_odd_stdev = statistics.stdev(losses_odd_list) if len(losses_odd_list) > 1 else 0.0\n",
    "                        integ_metrics_stdev = statistics.stdev(integ_metrics_list) if len(integ_metrics_list) > 1 else 0.0\n",
    "                        data[f'{dataset}_{F_type}_{model}_{config.G_type}_{gamma_ratio}_{met}'] = [dataset, model, F_type, G_type, gamma_ratio, met, metrics_mean, metrics_stdev, losses_even_ind_mean, losses_even_ind_stdev, losses_odd_ind_mean, losses_odd_ind_stdev, losses_ind_mean, losses_ind_stdev, losses_mean_mean, losses_mean_stdev, losses_even_mean, losses_even_stdev, losses_odd_mean, losses_odd_stdev, integ_metrics_mean, integ_metrics_stdev]\n",
    "\n",
    "\n",
    "        # DataFrameに変換\n",
    "        df_all = pd.DataFrame.from_dict(data, orient=\"index\", columns=[\"dataset\", \"model\", \"F_type\", \"G_type\", \"gamma_ratio\", \"metrics\", \"metrics_mean\", \"metrics_stdev\", \"even_ind_mean\", \"even_ind_stdev\", \"odd_ind_mean\", \"odd_ind_stdev\", \"ind_mean\", \"ind_stdev\", \"mean_mean\", \"mean_stdev\", \"even_mean\", \"even_stdev\", \"odd_mean\", \"odd_stdev\", \"integ_metrics_mean\", \"integ_metrics_stdev\"])\n",
    "        df_all.to_csv(output_path / f\"result_{dataset}_0912.csv\", index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "def partial_run():\n",
    "    logger.info(f\"データセット: {config.dataset}\")\n",
    "    \n",
    "    # datasetの読み込み\n",
    "    train_df, test_df = load_data(config=config)\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    # dim_intermediate,dim_integrate\n",
    "    F_types =[\"kernel_pca\"]#[\"svd\", \"kernel_pca\"]\n",
    "    G_types = []#, \"targetvec\", \"GEP\", \"GEP_weighted\"]\n",
    "    config.lambda_gen_eigen = 0.00001\n",
    "    #for F_type in F_types:\n",
    "    dim_intermediate = config.dim_intermediate\n",
    "    for dim_intermediate in range(1, dim_intermediate + 1, 5):\n",
    "        config.dim_intermediate = dim_intermediate\n",
    "        config.dim_integrate = config.dim_intermediate\n",
    "        for G_type in G_types:\n",
    "            config.F_type = F_types[0]\n",
    "            config.G_type = G_type\n",
    "            if config.F_type == \"kernel_pca\" and config.G_type == \"GEP_weighted\":\n",
    "                # GEP_weightedはUSE_KERNELがTrueのときのみ実行\n",
    "                return\n",
    "            if config.F_type == \"kernel_pca\" and config.G_type == \"GEP\":\n",
    "                # GEP_weightedはUSE_KERNELがTrueのときのみ実行\n",
    "                return\n",
    "            # インスタンスの生成\n",
    "            data_collaboration = DataCollaborationAnalysis(config=config, logger=logger, train_df=train_df, test_df=test_df)\n",
    "            # データ分割 -> 統合表現の獲得まで一気に実行\n",
    "            data_collaboration.run()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    name = \"main_loop\"\n",
    "    if name == \"main_loop\":\n",
    "        main_loop()\n",
    "    elif name == \"partial_run\":\n",
    "        partial_run()\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacollaborationanalysis-jS9KJoip-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
